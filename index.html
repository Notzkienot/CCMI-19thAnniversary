<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>CCMI 19th Anniversary ‚Äì Arced Head Tracking Filter</title>
  <style>
    /* --- Background --- */
    body {
      margin: 0;
      height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      font-family: "Poppins", system-ui, sans-serif;
      color: #fff;
      background:
        radial-gradient(circle at 15% 10%, rgba(255,215,0,0.22), transparent 38%),
        linear-gradient(135deg, #2b0047, #7b2cbf, #c77dff);
      background-size: 400% 400%;
      animation: bgMove 10s ease infinite;
      overflow: hidden;
      position: relative;
    }
    @keyframes bgMove {
      0% { background-position: 0% 50%; }
      50% { background-position: 100% 50%; }
      100% { background-position: 0% 50%; }
    }

    /* --- Camera Container --- */
    .camera-container {
      position: relative;
      width: 92vw;
      max-width: 420px;
      aspect-ratio: 3/4;
      border: 3px solid gold;
      border-radius: 16px;
      overflow: hidden;
      background: #000;
      box-shadow: 0 0 30px rgba(255, 215, 0, 0.45);
    }
    video, img.preview {
      width: 100%;
      height: 100%;
      object-fit: cover;
      display: block;
    }
    img.preview { display: none; }

    /* --- Canvas Overlay for Arc Text --- */
    #filterCanvas {
      position: absolute;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      pointer-events: none;
      z-index: 3;
    }

    /* --- Buttons --- */
    .controls {
      margin-top: 16px;
      display: flex;
      gap: 12px;
      z-index: 2;
    }
    button, a.button {
      background: linear-gradient(135deg, gold, #ffef9f);
      color: #4b0082;
      border: none;
      padding: 12px 28px;
      border-radius: 10px;
      font-size: 16px;
      font-weight: 700;
      cursor: pointer;
      text-decoration: none;
      box-shadow: 0 4px 10px rgba(0,0,0,0.35);
      transition: transform 0.15s ease, box-shadow 0.15s ease;
    }
    button:hover, a.button:hover {
      transform: scale(1.04);
      box-shadow: 0 0 14px rgba(255, 215, 0, 0.6);
    }
    #download, #retake { display: none; }

    .note {
      margin-top: 8px;
      font-size: 12px;
      color: #ffefc3;
      text-shadow: 0 0 4px #000;
      opacity: 0.9;
    }
  </style>
</head>
<body>
  <div class="camera-container" id="cameraBox">
    <video id="camera" autoplay playsinline></video>
    <canvas id="filterCanvas"></canvas>
    <img id="capturedImage" class="preview" alt="Captured" />
  </div>

  <div class="controls">
    <button id="capture">üì∏ Capture</button>
    <button id="retake">üîÑ Retake</button>
    <a id="download" class="button" href="#" download="photo.png">‚¨áÔ∏è Download</a>
  </div>

  <div class="note">Tip: Use HTTPS (Netlify, GitHub Pages, Vercel) for camera access.</div>

  <!-- face-api.js -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <script>
    const video = document.getElementById('camera');
    const imgPreview = document.getElementById('capturedImage');
    const captureBtn = document.getElementById('capture');
    const retakeBtn  = document.getElementById('retake');
    const downloadBtn = document.getElementById('download');
    const cameraBox = document.getElementById('cameraBox');
    const filterCanvas = document.getElementById('filterCanvas');
    const ctx = filterCanvas.getContext('2d');

    let stream;
    let runTrack = false;

    function lerp(a, b, t) { return a + (b - a) * t; }

    async function startCamera() {
      stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'user' },
        audio: false
      });
      video.srcObject = stream;
      await video.play();

      // Sync canvas with video size
      filterCanvas.width = cameraBox.clientWidth;
      filterCanvas.height = cameraBox.clientHeight;
    }

    async function loadModels() {
      await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
      await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
    }

    // Draw curved text around head
    function drawArcText(ctx, text, centerX, centerY, radius, startAngle, endAngle) {
      const chars = text.split('');
      const angleRange = endAngle - startAngle;
      const anglePerChar = angleRange / (chars.length - 1);

      ctx.save();
      ctx.translate(centerX, centerY);
      ctx.font = `bold ${Math.round(cameraBox.clientWidth * 0.05)}px Poppins`;
      ctx.textAlign = 'center';
      ctx.textBaseline = 'middle';

      const grad = ctx.createLinearGradient(-radius, 0, radius, 0);
      grad.addColorStop(0, '#2b0047');
      grad.addColorStop(0.5, '#7b2cbf');
      grad.addColorStop(1, '#c77dff');

      ctx.fillStyle = grad;
      ctx.shadowColor = 'rgba(255,215,0,0.6)';
      ctx.shadowBlur = 15;

      for (let i = 0; i < chars.length; i++) {
        const angle = startAngle + i * anglePerChar;
        ctx.save();
        ctx.rotate(angle);
        ctx.translate(0, -radius);
        ctx.fillText(chars[i], 0, 0);
        ctx.restore();
      }

      ctx.restore();
    }

    async function trackFace() {
      const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 224, scoreThreshold: 0.4 });
      let tx = cameraBox.clientWidth / 2;
      let ty = cameraBox.clientHeight / 2;

      const loop = async () => {
        if (!runTrack) return;
        const result = await faceapi
          .detectSingleFace(video, options)
          .withFaceLandmarks();

        ctx.clearRect(0, 0, filterCanvas.width, filterCanvas.height);

        if (result && result.landmarks) {
          const leftEye = result.landmarks.getLeftEye();
          const rightEye = result.landmarks.getRightEye();
          const cx = (leftEye[0].x + rightEye[3].x) / 2;
          const cy = (leftEye[0].y + rightEye[3].y) / 2;

          tx = lerp(tx, cx, 0.25);
          ty = lerp(ty, cy - 40, 0.25);

          const scaleX = cameraBox.clientWidth / video.videoWidth;
          const scaleY = cameraBox.clientHeight / video.videoHeight;

          const x = tx * scaleX;
          const y = ty * scaleY;

          drawArcText(ctx, 'CCMI 19th Anniversary', x, y - 20, 120, -Math.PI / 1.3, Math.PI / 1.3);
        }
        requestAnimationFrame(loop);
      };
      requestAnimationFrame(loop);
    }

    async function init() {
      try {
        await loadModels();
        await startCamera();
        runTrack = true;
        trackFace();
      } catch (e) {
        alert('Camera or model load failed. Ensure HTTPS and models/ are in place.');
        console.error(e);
      }
    }

    init();

    // --- Capture with filter ---
    captureBtn.addEventListener('click', () => {
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      const ctx2 = canvas.getContext('2d');

      // Draw current camera frame
      ctx2.drawImage(video, 0, 0, canvas.width, canvas.height);

      // Map face position to canvas scale
      const boxW = cameraBox.clientWidth;
      const boxH = cameraBox.clientHeight;
      const scaleX = canvas.width / boxW;
      const scaleY = canvas.height / boxH;

      // Extract the latest position of the arc from the overlay
      const radius = 120 * scaleY;
      const grad = ctx2.createLinearGradient(0, 0, canvas.width, canvas.height);
      grad.addColorStop(0, '#2b0047');
      grad.addColorStop(0.5, '#7b2cbf');
      grad.addColorStop(1, '#c77dff');

      // Use the latest face-tracked coordinates (approx from middle)
      const x = boxW / 2 * scaleX;
      const y = boxH / 2 * scaleY - 120;

      ctx2.save();
      ctx2.translate(x, y);
      ctx2.font = `bold ${Math.round(boxW * 0.05 * scaleX)}px Poppins`;
      ctx2.textAlign = 'center';
      ctx2.textBaseline = 'middle';
      ctx2.fillStyle = grad;
      ctx2.shadowColor = 'rgba(255,215,0,0.6)';
      ctx2.shadowBlur = 15;

      const text = 'CCMI 19th Anniversary';
      const chars = text.split('');
      const startAngle = -Math.PI / 1.3;
      const endAngle = Math.PI / 1.3;
      const angleRange = endAngle - startAngle;
      const anglePerChar = angleRange / (chars.length - 1);

      for (let i = 0; i < chars.length; i++) {
        const angle = startAngle + i * anglePerChar;
        ctx2.save();
        ctx2.rotate(angle);
        ctx2.translate(0, -radius);
        ctx2.fillText(chars[i], 0, 0);
        ctx2.restore();
      }
      ctx2.restore();

      const dataUrl = canvas.toDataURL('image/png');
      imgPreview.src = dataUrl;
      imgPreview.style.display = 'block';
      video.style.display = 'none';

      downloadBtn.href = dataUrl;
      downloadBtn.style.display = 'inline-block';
      retakeBtn.style.display = 'inline-block';
      captureBtn.style.display = 'none';

      runTrack = false;
    });

    retakeBtn.addEventListener('click', async () => {
      imgPreview.style.display = 'none';
      video.style.display = 'block';
      downloadBtn.style.display = 'none';
      retakeBtn.style.display = 'none';
      captureBtn.style.display = 'inline-block';

      if (!stream) await startCamera();
      runTrack = true;
      trackFace();
    });
  </script>
</body>
</html>
